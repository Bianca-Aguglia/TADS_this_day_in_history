{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 2)</small>\n",
    "---\n",
    "**Goal:** create a dataset of this-day-in-history events  \n",
    "  \n",
    "**Context:**  I need a starting dataset of world history events (name, short description, long description, image, and location). I couldn't find public datasets. I'm building one using the wikipedia this day in history data.\n",
    "\n",
    "**Notes about this notebook:**  \n",
    "- this notebook is for the second step of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_api_step_01_02_get_data_29jan21](http://localhost:8888/notebooks/temp_for_offline/bianca_aguglia/projects_wip/TADS_wikipedia_this_day_in_history/TADS_wikipedia_tdih_main_api_step_01_02_get_data_29jan21.ipynb)\n",
    "- the first step consisted of:\n",
    "    - using the Wikipedia api to get the events for each day of the year\n",
    "    - parsing the Wikipedia data, cleaning it up, and getting it in the right format needed for the SQLite database created for this project\n",
    "- step two is:\n",
    "    - take the data from step one and rank each item based on page views, page length, and links to page\n",
    "    - get image (and licence details) for each item in the data from step one\n",
    "- to get data for a specific day run day_data_main(day_name). It returns a dictionary with day data.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow: <small>(with details and checkmarks for steps done in this notebook)</small>\n",
    "- get day data\n",
    "    - save data in wikipedia_tdih.db\n",
    "- get link data\n",
    "    - [ ] query wikipedia_tdih.db for:\n",
    "        - [x] links that are new (i.e. in wiki_link but not in wiki_get_link_data_log)\n",
    "        - [x] links that have have not been updated since a specified_date\n",
    "    - [ ] get link data from wikipedia API and extract the following:\n",
    "        - page_size\n",
    "        - incoming_links\n",
    "        - coordinates\n",
    "        - page_score\n",
    "        - first_paragraphs\n",
    "        - image_url\n",
    "        - image_file\n",
    "        - wikibase_shortdesc\n",
    "        - wikibase_item\n",
    "        - wiki_desc\n",
    "        - page_views\n",
    "    - [ ] add link data to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import time\n",
    "import config\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = 'wikipedia_tdih.db'\n",
    "DOE = dt.date.today().strftime('%Y-%m-%d')\n",
    "URL = 'https://en.wikipedia.org/w/api.php/'\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'cirrusbuilddoc|cirruscompsuggestbuilddoc|extracts|pageimages|pageprops|pageterms|pageviews',\n",
    "          'piprop': 'original',\n",
    "          'exintro': True # gives the first paragraphs of a page (before detail sections)\n",
    "         }\n",
    "# explanation of values for 'prop'\n",
    "# cirrusbuilddoc for text_bytes (aka page_size), incoming_link, coordinates (if any)\n",
    "# cirruscompsuggestbuilddoc for score\n",
    "# extracts (+ exintro) for first_paragraph(s) (regex to clean up) (might me better to use ['cirrusbuilddoc']['text_source'])\n",
    "# pageimages for pageimage (with piprop='original') for full size image (but doesn't return image file anymore)\n",
    "# pageprops for page_image_free, wikibase_short_description, wikibase_item\n",
    "# pageterms for description (similar but not identital to wikibase_short_description)\n",
    "# pageviews for pageviews :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the link data for a group of links in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_links_to_update_from_db\n",
    "        - get_link_data\n",
    "        - extract_link_data\n",
    "        - add_link_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file:\n",
    "        date: \n",
    "        batch_size: no. of links to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(DATABASE_FILE) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the links we need data for\n",
    "        links_list = get_links_to_update_from_db(cursor, date)\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not links_list:\n",
    "            return 'no link data needs to be retrieved / updated'\n",
    "        \n",
    "        # keep track of failed_request (break if failed_requests > 5)\n",
    "        failed_request = 0\n",
    "    \n",
    "        # process links in batches of size batch_size\n",
    "        for i in range(0, len(links_list), batch_size):\n",
    "            link_batch = links_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # get link data for links in batch\n",
    "            resp = get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS)\n",
    "            \n",
    "            # extract link_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                page_dicts = extract_link_data(resp)\n",
    "\n",
    "            else:\n",
    "                # update wiki_link_data_log with response status_code\n",
    "#                 update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE)\n",
    "                update_table_wiki_link_data_log(link_batch, resp.status_code)\n",
    "                failed_request += 1\n",
    "                if failed_request % 5 == 0:\n",
    "                    return f'failed requests: {failed_request}'\n",
    "                continue\n",
    "\n",
    "            # add link_data to db\n",
    "            for page_dict in page_dicts:\n",
    "                add_link_data_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_update_from_db(database_cursor, date = ''):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the links for which wikipedia data is needed.\n",
    "    \n",
    "    There are two cases in which data is needed for a specific link:\n",
    "        1. the link has just been added to the wikipedia_tdih.db and link data has not been requested from \n",
    "           wikipedia API yet\n",
    "        2. the existing data for the link needs to be updated\n",
    "    \n",
    "    Params:\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "              \n",
    "    Returns:\n",
    "        links_list: list of links for which wikpedia data is needed\n",
    "    \"\"\"\n",
    "    \n",
    "    c = database_cursor\n",
    "    \n",
    "    # if no date is given\n",
    "    if not date:\n",
    "        # select the links that are in wiki_link but not in wiki_get_link_data_log\n",
    "        # (these are links which have just been added to wiki_link)\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id NOT IN (\n",
    "                                    SELECT link_id FROM wiki_get_link_data_log) ''').fetchall()\n",
    "    \n",
    "    # if date is given\n",
    "    else:\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id IN (\n",
    "                                    SELECT link_id FROM wiki_get_link_data_log WHERE doe > ?)''', (date,)).fetchall()\n",
    "      \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get link data from Wikipedia API. Links are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        link_batch: list of links to get data for (usually a batch of size 5)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "    \"\"\"\n",
    "    # join the links in link_batch and assign them to request params\n",
    "#     titles = '|'.join(link_batch)\n",
    "    titles = '|'.join(wiki_link[1] for wiki_link in link_batch)\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_data(response):\n",
    "    \"\"\"\n",
    "    Extract link data\n",
    "    \n",
    "    Params:\n",
    "        response: response data received from Wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        page_dicts: list of dictionaries with page data\n",
    "    \n",
    "    \"\"\"\n",
    "    resp = response.json()\n",
    "    status_code = response.status_code\n",
    "    page_dicts = []\n",
    "    \n",
    "    # get the wiki_links from the response (to match them to wiki_link in database)\n",
    "    wiki_links_list = resp['query']['normalized']\n",
    "    wiki_links_dict = {link['to']: link['from'] for link in wiki_links_list}\n",
    "    \n",
    "    for page_id in resp['query']['pages'].keys():\n",
    "        page_dict = resp['query']['pages'][page_id]\n",
    "        \n",
    "        # wiki_link is normalized by Wikipedia (e.g. 'Albert_Einstein' normalized to 'Albert Einstein')\n",
    "        # extract it and find its original value\n",
    "        wiki_link_normalized = page_dict['title']\n",
    "        wiki_link = wiki_links_dict[wiki_link_normalized]\n",
    "        \n",
    "        # build up the data dict\n",
    "        data_dict = {'wiki_link': wiki_link,\n",
    "                     'status_code': status_code,\n",
    "                     'page_size': page_dict['cirrusbuilddoc']['text_bytes'],\n",
    "                     'incoming_links': page_dict['cirrusbuilddoc']['incoming_links'],\n",
    "                     'coordinates': page_dict['cirrusbuilddoc']['coordinates'],\n",
    "                     'page_score': page_dict['cirruscompsuggestbuilddoc'][f'{page_id}t']['score_explanation']['value'],\n",
    "                     'first_paragraphs': BeautifulSoup(page_dict['extract']).text.strip(),\n",
    "                     'image_url': page_dict['original']['source'],\n",
    "                     'image_file': page_dict['pageprops']['page_image_free'],\n",
    "                     'wikibase_shortdesc': page_dict['pageprops']['wikibase-shortdesc'],\n",
    "                     'wikibase_item': page_dict['pageprops']['wikibase_item'],\n",
    "                     'wiki_desc': page_dict['terms']['description'],\n",
    "                     'page_views': page_dict['pageviews']}\n",
    "        page_dicts.append(data_dict)\n",
    "        \n",
    "    return page_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_link_data_to_db(page_data_dict, db = DATABASE_FILE, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add wiki_link data to wikipedia_tdih.db\n",
    "    \"\"\"\n",
    "    # connect to wikipedia_tdih.db\n",
    "    with sqlite3.connect(db) as conn:\n",
    "        c = conn.cursor()\n",
    "    \n",
    "        # get link_id from db\n",
    "        c.execute('SELECT link_id, link_url FROM wiki_link WHERE link_url = ?', page_data_dict[wiki_link])\n",
    "        link_id = c.fetchone()[0]\n",
    "    \n",
    "        # update tables\n",
    "        update_table_wiki_link_data_log(link_id, page_data_dict['status_code'], c, doe)\n",
    "        update_table_wiki_link_size(link_id, page_data_dict['page_size'], c, doe)\n",
    "        update_table_wiki_link_page_views(link_id, page_data_dict['page_views'], c, doe)\n",
    "        update_table_wiki_link_info(link_id, page_data_dict, c, doe)\n",
    "        update_table_wiki_image()\n",
    "        update_table_wiki_image_usage()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
