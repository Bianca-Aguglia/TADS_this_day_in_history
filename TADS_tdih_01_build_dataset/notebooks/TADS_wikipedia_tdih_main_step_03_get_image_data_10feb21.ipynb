{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 3 - get image data)</small>\n",
    "---\n",
    "**Goal:** get image data for images in wikipedia_tdih.db (if no data already in database).\n",
    "\n",
    "**Notes about this notebook:**  \n",
    "- this notebook is for the third step of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_step_01_get_day_data_30jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)\n",
    "- the notebook for the second step is [TADS_wikipedia_tdih_main_step_02_get_link_data_07feb21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow summary: <small>(with details and checkmarks for steps done in this notebook)</small>\n",
    "1. get day data\n",
    "    - get day data from wikipedia API, process it, and save it to wikipedia_tdih.db\n",
    "2. get link data\n",
    "    - get link data from wikipedia API, process it, and save in to wikipedia_tdih.db\n",
    "3. get image data\n",
    "    - [ ] query wikipedia_tdih.db for images that are in wiki_image but not in wiki_image_data_log\n",
    "    - [ ] get image data from wikipedia API and extract the following:\n",
    "        - [ ] user (used for wiki_credit)\n",
    "        - [ ] image_url ?\n",
    "        - [ ] copyright license\n",
    "        - [ ] license name\n",
    "        - [ ] license description\n",
    "        - [ ] license usage rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "import config\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = config.DATABASE_FILE\n",
    "DOE = config.DOE\n",
    "URL = config.WIKIPEDIA_URL\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'imageinfo',\n",
    "          'iiprop': 'user|url|extmetadata', \n",
    "         }\n",
    "\n",
    "# explanation of values for 'iiprop'\n",
    "# user for wikipedia_user\n",
    "# url for wikipedia_url for full picture\n",
    "# extmetadata for license and attribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE, print_progress = 0,\n",
    "                    sleep_range = (5,10), url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the image data for a group of images in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_images_to_update_from_db\n",
    "        - get_image_data\n",
    "        - extract_image_data\n",
    "        - add_image_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file: database_file to read from, update, and save data to\n",
    "        date: default of '' indicates that only images without data should be selected\n",
    "              if date is given, select the images that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "        batch_size: no. of images to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        doe: date of entry (defaults to current day)\n",
    "        print_progress: if different than 0, prints batch number being processed (every print_progress no. of batches)\n",
    "        sleep_range: tuple of integers indicating the lower_limit and upper_limit of how long to wait between\n",
    "                     API calls\n",
    "        url: url to use for request to wikipedia API\n",
    "        params: params to use in request to wikipedia API\n",
    "        headers: headers to use in request to wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        image_list: list of images for which Wikipedia data is needed\n",
    "        failed_request: how many requests to Wikipedia API didn't have requests.codes.ok (break at 5)\n",
    "        missing_image_data: list of images no data was received for\n",
    "        update_results: a list of dictionaries with the status of each table update\n",
    "                        e.g. {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_table_name,\n",
    "                              'update_status': 'update_complete',\n",
    "                              'update_note': np.nan}\n",
    "    \"\"\"\n",
    "    # keep track of images no data was received for and of failed requests\n",
    "    missing_image_data = []\n",
    "    failed_request = 0\n",
    "    resp_list = []\n",
    "    \n",
    "    # initialize update_results\n",
    "    update_results = []\n",
    "    \n",
    "    with sqlite3.connect(database_file) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the image we need data for\n",
    "        image_list = get_images_to_update_from_db(cursor)\n",
    "        \n",
    "        # if print_progress\n",
    "        if print_progress:\n",
    "            print(f'images to update: {len(image_list)}')\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not image_list:\n",
    "            print('no image data needs to be retrieved / updated')\n",
    "            return ((np.nan,) * 5)\n",
    "        \n",
    "        image_list = image_list[:7] # this is just for testing purposes\n",
    "    \n",
    "        # process images in batches of size batch_size\n",
    "        for i in range(0, len(image_list), batch_size):\n",
    "            image_batch = image_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # if print_progress is given\n",
    "            if print_progress:\n",
    "                if (i+1) % print_progress == 0:\n",
    "                    print(f'processing batch {i+1} of {len(image_list)}')\n",
    "            \n",
    "            # get image data for images in batch\n",
    "            resp = get_image_data(image_batch, url = url, params = params, headers = headers)\n",
    "            resp_list.append(resp.json()) # this is just for testing\n",
    "            \n",
    "            # extract image_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                image_dicts, missing_batch_data = extract_image_data(resp)                \n",
    "                            \n",
    "                # if missing_batch_data list is not empty, update missing_image_data\n",
    "                missing_image_data.extend(missing_batch_data)\n",
    "\n",
    "            else:\n",
    "                # for each image, update wiki_image_data_log with response status_code\n",
    "                for image in image_batch:\n",
    "                    update_results.append(update_table_wiki_image_data_log(image_batch[0], \n",
    "                                                                           resp.status_code, \n",
    "                                                                           cursor, \n",
    "                                                                           doe))\n",
    "                \n",
    "                failed_request += 1\n",
    "                if failed_request == 5:\n",
    "                    print('failed requests limit reached.')\n",
    "                    return image_list, failed_request, missing_image_data, update_results, resp_list\n",
    "                continue\n",
    "\n",
    "            # add image_data to db and update update_results list\n",
    "            for image_dict in image_dicts:\n",
    "                update_results.extend(add_image_data_to_db(image_dict, cursor, doe))\n",
    "                \n",
    "            # sleep time before the next API call\n",
    "            time.sleep(round(random.uniform(sleep_range[0], sleep_range[1]), 2))\n",
    "            \n",
    "        cursor.close()\n",
    "                \n",
    "    return image_list, failed_request, missing_image_data, update_results, resp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_to_update_from_db(database_cursor, date = ''):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the images for which data is needed. \n",
    "    \n",
    "    There are two cases in which data is needed for a specific image:\n",
    "        1. the image has just been added to the wikipedia_tdih.db and image data has not been requested from \n",
    "           wikipedia API yet\n",
    "        2. the existing data for the image needs to be updated\n",
    "    \n",
    "    Params:\n",
    "        database_cursor: database cursor\n",
    "        date: default of '' indicates that only images without data should be selected\n",
    "              if date is given, select the images that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "    in wiki_image_data_log.\n",
    "        \n",
    "    Returns:\n",
    "        image_list: list of images for which wikipedia data is needed.\n",
    "    \"\"\"\n",
    "    cursor = database_cursor\n",
    "    \n",
    "    # if no date is given\n",
    "    if not date:\n",
    "    \n",
    "        image_list = cursor.execute('''SELECT image_id, image_file FROM wiki_image WHERE image_id NOT IN (\n",
    "                                        SELECT image_id FROM wiki_image_info )''').fetchall()\n",
    "    \n",
    "    # if date is given\n",
    "    else:\n",
    "        image_list = cursor.execute('''SELECT image_id, image_file FROM wiki_image WHERE image_id IN (\n",
    "                                       SELECT image_id FROM wiki_image_data_log WHERE doe > ?)''', (date,)).fetchall()\n",
    "      \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(image_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get image data from Wikipedia API. Images are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        image_batch: list of tuples representing images to get data for (usually a batch of size 5)\n",
    "                     each tuple is of the form (image_id, image_file)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "        headers: headers to use in API call\n",
    "        \n",
    "    Returns:\n",
    "        resp: requests response object\n",
    "    \"\"\"\n",
    "    # join the image_files for images in image_batch and assign them to request params\n",
    "    titles = '|'.join(f'File:{wiki_image[1]}' for wiki_image in image_batch)\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_data(response):\n",
    "    \"\"\"\n",
    "    Extract image data for a batch of images.\n",
    "    \n",
    "    Params:\n",
    "        response: requests reponse from Wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        image_dict_list: list of dictionaries with data for each image in the batch.\n",
    "                         e.g. see image_dict below        \n",
    "    \"\"\"\n",
    "    resp = response.json()\n",
    "    image_dict_list = [] \n",
    "    missing_image_data = []\n",
    "    \n",
    "    # wikipedia normalizes file names (e.g. 'File:François_Ier_Louvre.jpg ' to 'File:François Ier Louvre.jpg')\n",
    "    # keep track of original vs. normalized files names (to match to file name in wikipedia_tdih.db)\n",
    "    file_names = resp['query']['normalized']\n",
    "    file_names_dict = {file['to']:file['from'] for file in file_names}\n",
    "    \n",
    "    # fields that, if available, are dictionaries from which data in ['value'] needs to be extracted\n",
    "    fields_with_value = ['image_credit','image_description','image_license_name', 'image_usage_terms', \n",
    "                         'image_attrib_required', 'image_copyright', 'image_restriction', 'image_license']\n",
    "    \n",
    "    # image data is dictionary resp['query']['pages'] where each key has the data for one image\n",
    "    for image in resp['query']['pages'].values():\n",
    "        \n",
    "        # if image doesn't have imageinfo, add to missing_image_data\n",
    "        if 'imageinfo' not in image.keys():\n",
    "            missing_image_data.append(image['title'])\n",
    "            continue\n",
    "            \n",
    "        # else, build up image_dict\n",
    "        image_dict = {'title' : file_names_dict.get(image['title'], image['title']).replace('File:',''), # if image title was not normalized\n",
    "                      'title_normalized': image['title'].replace('File:',''),\n",
    "                      'image_repository' : image['imagerepository'],\n",
    "                      'user' : image['imageinfo'][0]['user'],\n",
    "                      'image_url' : image['imageinfo'][0]['url'],\n",
    "                      'image_date' : image['imageinfo'][0]['extmetadata']['DateTime']['value'],\n",
    "                      'image_credit' : image['imageinfo'][0]['extmetadata'].get('Credit', np.nan),\n",
    "                      'image_description' : image['imageinfo'][0]['extmetadata'].get('ImageDescription', np.nan),\n",
    "                      'image_license_name' : image['imageinfo'][0]['extmetadata'].get('LicenseShortName', np.nan),\n",
    "                      'image_usage_terms' : image['imageinfo'][0]['extmetadata'].get('UsageTerms', np.nan),\n",
    "                      'image_attrib_required' : image['imageinfo'][0]['extmetadata'].get('AttributionRequired', np.nan),\n",
    "                      'image_copyright' : image['imageinfo'][0]['extmetadata'].get('Copyrighted', np.nan),\n",
    "                      'image_restriction' : image['imageinfo'][0]['extmetadata'].get('Restrictions', np.nan),\n",
    "                      'image_license' : image['imageinfo'][0]['extmetadata'].get('License', np.nan)}\n",
    "        \n",
    "        # if available, add data from 'value'\n",
    "        for field in fields_with_value:\n",
    "            if isinstance(image_dict[field], dict):\n",
    "                image_dict[field] = image_dict[field]['value']\n",
    "\n",
    "        # if image has image_description it is usually html data \n",
    "        # extract text for image_description\n",
    "        if isinstance(image_dict['image_description'], str):\n",
    "            image_dict['image_description'] = BeautifulSoup(image_dict['image_description']).text\n",
    "            \n",
    "        # standardize the data in image_attrib_required and image_copyright (sample values are 'False', 'false', 'True', etc)\n",
    "        for str_data in ['image_attrib_required', 'image_copyright']:\n",
    "            image_dict[str_data] = image_dict[str_data].strip().lower()\n",
    "    \n",
    "        image_dict_list.append(image_dict)\n",
    "        \n",
    "    return image_dict_list, missing_image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# for update_results: is it better to only log failures\n",
    "def add_image_data_to_db(image_dict, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add image data to wikipedia_tdih.db\n",
    "    \n",
    "    Params:\n",
    "        image_dict: dictionary of data to be added to the database\n",
    "        database_cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        update_results: a list of dictionaries with the status of each table update\n",
    "                        e.g. {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_event',\n",
    "                              'update_status': 'update_complete',\n",
    "                              'update_note': 'events'}\n",
    "        \n",
    "    \"\"\"\n",
    "    update_results = []\n",
    "    \n",
    "\n",
    "    # get license_id (update table wiki_license first, if license_id not present)\n",
    "    license_data = (image_dict['image_license_name'], image_dict['image_usage_terms'],\n",
    "                    image_dict['image_attrib_required'], image_dict['image_copyright'])\n",
    "    license_id, update_result = update_table_wiki_license(license_data, cursor, doe)\n",
    "    # if wiki_license was updated, update_result is a non-emtpy dict\n",
    "    # TO-DO: update_result is not-emtpy also if update_table_wiki_license failed => need to handle that\n",
    "    if update_result:\n",
    "        update_results.append(update_result)\n",
    "\n",
    "    # get user_id (update table wiki_user first, if user_id not present) \n",
    "    user_id, update_result = update_table_wiki_user(image_dict['user'], cursor, doe)\n",
    "    # TO-DO: action for when update_result is not empty => failure to update wiki_user => user_id = ''\n",
    "    if update_result:\n",
    "        update_results.append(update_result)\n",
    "\n",
    "    # get image_id\n",
    "    cursor.execute('SELECT image_id FROM wiki_image WHERE image_url = ?', (image_dict['image_url'],))\n",
    "    image_id = cursor.fetchone()[0]\n",
    "\n",
    "    # update table wiki_image_info\n",
    "    image_data = (doe, image_id, license_id, user_id,\n",
    "                  image_dict['image_repository'], image_dict['image_date'], image_dict['image_credit'],\n",
    "                  image_dict['image_description'])\n",
    "    update_results.append(update_table_wiki_image_info(image_data, cursor, doe))\n",
    "    \n",
    "    return update_results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def update_table_wiki_image_data_log(link_id, status_code, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_data_log\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of link to be updated in wiki_link_data_log\n",
    "        status_code: requests.status_code from Wikipedia API\n",
    "        cursor: database_cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "    \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = (doe, link_id, status_code)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('INSERT INTO wiki_link_data_log VALUES (null,?,?,?)', data)\n",
    "        update_status = 'update_complete'\n",
    "    \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_link_data_log',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "        \n",
    "    return update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image_info(image_data, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image_info.\n",
    "    \n",
    "    Params:\n",
    "        image_data: tuple with data for image\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    update_status_dict = {}\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('INSERT INTO wiki_image_info VALUES (null,?,?,?,?,?,?,?,?)', image_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        update_status_dict = {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_image_info',\n",
    "                              'update_status': update_status,\n",
    "                              'update_note': image_data[1]} # image_data[1] is the image_id\n",
    "        \n",
    "    return update_status_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# update_status for when user already in wiki_user\n",
    "# is it better to only log failures?\n",
    "def update_table_wiki_user(user_name, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update (if needed) table wiki_user\n",
    "    \n",
    "    Params:\n",
    "        user_name: wiki_user_name\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        user_id\n",
    "        update_status_dict: dictionary with status of wiki_user update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_user',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': user_name} \n",
    "                            this is an empty dictionary if user was already in wiki_user\n",
    "    \"\"\"\n",
    "    update_status_dict = {}\n",
    "    user_id = ''\n",
    "    \n",
    "    try:\n",
    "        # add user to wiki_user (if not already in database)\n",
    "        cursor.execute('INSERT or IGNORE INTO wiki_user VALUES (null,?,?)', (doe, user_name))\n",
    "\n",
    "        # get user_id\n",
    "        cursor.execute('SELECT user_id FROM wiki_user WHERE user_name = ?', (user_name,))\n",
    "        user_id = cursor.fetchone()[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        update_status_dict = {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_user',\n",
    "                              'update_status': update_status,\n",
    "                              'update_note': user_name}\n",
    "        \n",
    "    return user_id, update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_license(license_data, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update (if needed) table wiki_license)\n",
    "    \n",
    "    Params:\n",
    "        license_data:\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        license_id: license_id\n",
    "        update_status_dict: dictionary with status of wiki_license update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_license',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': license_data} \n",
    "                            this is an empty dictionary if license was already in wiki_license\n",
    "    \"\"\"\n",
    "    update_status_dict = {}\n",
    "    license_id = ''\n",
    "    \n",
    "    cursor.execute('''SELECT license_id FROM wiki_license WHERE \n",
    "                        license_name = ? AND\n",
    "                        license_description = ? AND\n",
    "                        attrib_required = ? AND\n",
    "                        copyright = ?''', license_data)\n",
    "    \n",
    "    try:\n",
    "        # get the license_id (if successful, update_status_dict stays empty dictionary)\n",
    "        license_id = cursor.fetchone()[0]\n",
    "\n",
    "        return license_id, update_status_dict\n",
    "    \n",
    "    except:\n",
    "        # insert the new license type into wiki_copyright_license\n",
    "        try:\n",
    "            license_data = (doe, *license_data)\n",
    "            cursor.execute('INSERT INTO wiki_license VALUES (null, ?,?,?,?,?)', license_data)\n",
    "            license_id = cursor.lastrowid\n",
    "            update_status = 'update_complete'\n",
    "        \n",
    "        except Exception as e:\n",
    "            update_status = repr(e)\n",
    "\n",
    "        update_status_dict = {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_license',\n",
    "                              'update_status': update_status,\n",
    "                              'update_note': license_data}\n",
    "    \n",
    "    return license_id, update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images to update: 25\n",
      "processing batch 1 of 7\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "processing batch 6 of 7\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n",
      "in wiki_license\n",
      "in wiki_user\n",
      "in wiki_image_info\n"
     ]
    }
   ],
   "source": [
    "il, fr, mid, ur, resp = image_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, \n",
    "                                  doe = DOE, print_progress = 1, sleep_range = (5,10),\n",
    "                                  url = URL, params = PARAMS, headers = HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 'Evstafiev-sarajevo-building-burns.jpg'),\n",
       " (29, 'Flag_of_Haiti.svg'),\n",
       " (14, 'Flag_of_Morocco.svg'),\n",
       " (2, 'Flag_of_South_Carolina.svg'),\n",
       " (18, 'Flag_of_Vietnam.svg'),\n",
       " (1, 'Flag_of_the_Czech_Republic.svg'),\n",
       " (16, 'Gordie_Howe_Chex_card.jpg')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_list, failed_request, missing_image_data, update_results\n",
    "il, fr, mid, ur = image_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, \n",
    "                                  doe = DOE, print_progress = 0, sleep_range = (5,10),\n",
    "                                  url = URL, params = PARAMS, headers = HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, '1941hattie.jpg'),\n",
       " (11, 'A_Finnish_Maxim_M-32_machine_gun_nest_during_the_Winter_War.jpg'),\n",
       " (24, 'ApartheidSignEnglishAfrikaans.jpg'),\n",
       " (21, 'Archbishop-Tutu-medium.jpg'),\n",
       " (8, 'Berkeley-downtown-Bay-bridge-SF-in-back-from-Lab.jpg'),\n",
       " (28, 'Boeing_737-222,_Braniff_(American_Airlines)_AN0203004.jpg')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1941hattie.jpg',\n",
       " 'A Finnish Maxim M-32 machine gun nest during the Winter War.jpg',\n",
       " 'ApartheidSignEnglishAfrikaans.jpg',\n",
       " 'Archbishop-Tutu-medium.jpg',\n",
       " 'Berkeley-downtown-Bay-bridge-SF-in-back-from-Lab.jpg',\n",
       " 'Boeing 737-222, Braniff (American Airlines) AN0203004.jpg']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
