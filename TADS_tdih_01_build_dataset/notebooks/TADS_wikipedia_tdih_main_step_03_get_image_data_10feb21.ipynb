{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 3 - get image data)</small>\n",
    "---\n",
    "**Goal:** get image data for images in wikipedia_tdih.db (if no data already in database).\n",
    "\n",
    "**Notes about this notebook:**  \n",
    "- this notebook is for the third step of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_step_01_get_day_data_30jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)\n",
    "- the notebook for the second step is [TADS_wikipedia_tdih_main_step_02_get_link_data_07feb21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow summary: <small>(with details and checkmarks for steps done in this notebook)</small>\n",
    "1. get day data\n",
    "    - get day data from wikipedia API, process it, and save it to wikipedia_tdih.db\n",
    "2. get link data\n",
    "    - get link data from wikipedia API, process it, and save in to wikipedia_tdih.db\n",
    "3. get image data\n",
    "    - [ ] query wikipedia_tdih.db for images that are in wiki_image but not in wiki_image_data_log\n",
    "    - [ ] get image data from wikipedia API and extract the following:\n",
    "        - [ ] user (used for wiki_credit)\n",
    "        - [ ] image_url ?\n",
    "        - [ ] copyright license\n",
    "        - [ ] license name\n",
    "        - [ ] license description\n",
    "        - [ ] license usage rights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import sqlite3\n",
    "import config\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = config.DATABASE_FILE\n",
    "DOE = config.DOE\n",
    "URL = config.WIKIPEDIA_URL\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'imageinfo',\n",
    "          'iiprop': 'user|url|extmetadata', \n",
    "         }\n",
    "\n",
    "# explanation of values for 'iiprop'\n",
    "# user for wikipedia_user\n",
    "# url for wikipedia_url for full picture\n",
    "# extmetadata for license and attribution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE,\n",
    "                    url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the image data for a group of images in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_images_to_update_from_db\n",
    "        - get_image_data\n",
    "        - extract_image_data\n",
    "        - add_image_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file: database_file to read and update data from\n",
    "        date: if present, select images updated prior to date\n",
    "        batch_size: no. of links to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(DATABASE_FILE) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the image we need data for\n",
    "        image_list = get_images_to_update_from_db(cursor)\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not image_list:\n",
    "            return 'no image data needs to be retrieved / updated'\n",
    "        \n",
    "        # keep track of failed requests (break if failed_requests > 5)\n",
    "        failed_request = 0\n",
    "    \n",
    "        # process images in batches of size batch_size\n",
    "        for i in range(0, len(image_list), batch_size):\n",
    "            image_batch = image_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # get image data for images in batch\n",
    "            resp = get_image_data(image_batch, url = url, params = params, headers = headers)\n",
    "            \n",
    "            # extract link_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                image_dicts = extract_image_data(resp)\n",
    "\n",
    "            else:\n",
    "                # update wiki_image_data_log with response status_code\n",
    "#                 update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE)\n",
    "                update_table_wiki_image_data_log(image_batch, resp.status_code)\n",
    "                failed_request += 1\n",
    "                if failed_request == 5:\n",
    "                    return f'failed requests: {failed_request}'\n",
    "                continue\n",
    "\n",
    "            # add image_data to db\n",
    "            for image_dict in image_dicts:\n",
    "                add_image_data_to_db(image_dict, cursor, doe = doe)\n",
    "                \n",
    "        return 'wiki_image update complete'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_to_update_from_db(cursor):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the images for which data is needed. These images are in wiki_image but not\n",
    "    in wiki_image_data_log)\n",
    "    \n",
    "    Params:\n",
    "        cursor: database cursor\n",
    "        \n",
    "    Returns:\n",
    "        image_list: list of images for which wikipedia data is needed.\n",
    "    \"\"\"\n",
    "    image_list = cursor.execute('''SELECT image_id, image_file FROM wiki_image WHERE image_id NOT IN (\n",
    "                                    SELECT image_id FROM wiki_image_data_log )''').fetchall()\n",
    "    \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_data(image_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get image data from Wikipedia API. Images are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        image_batch: list of tuples representing images to get data for (usually a batch of size 5)\n",
    "                     each tuple is of the form (image_id, image_file)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "        headers: headers to use in API call\n",
    "        \n",
    "    Returns:\n",
    "        resp: requests response object\n",
    "    \"\"\"\n",
    "    # join the image_files for images in image_batch and assign them to request params\n",
    "    titles = '|'.join(wiki_image[1] for wiki_image in image_batch)\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_data(response):\n",
    "    \"\"\"\n",
    "    Extract image data for a batch of images.\n",
    "    \n",
    "    Params:\n",
    "        response: requests reponse from Wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        image_dict_list: list of dictionaries with data for each image in the batch.\n",
    "                         e.g. see image_dict below        \n",
    "    \"\"\"\n",
    "    resp = response.json()\n",
    "    image_dict_list = [] \n",
    "    \n",
    "    # wikipedia normalizes file names (e.g. 'File:François_Ier_Louvre.jpg ' to 'File:François Ier Louvre.jpg')\n",
    "    # keep track of original vs. normalized files names (to match to file name in wikipedia_tdih.db)\n",
    "    file_names = resp['query']['normalized']\n",
    "    file_names_dict = {file['to']:file['from'] for file in file_names}\n",
    "    \n",
    "    # fields that, if available, are dictionaries from which data in ['value'] needs to be extracted\n",
    "    fields_with_value = ['image_description','image_license_name', 'image_usage_terms', \n",
    "                         'image_attrib_required', 'image_copyright', 'image_restriction', 'image_license']\n",
    "    \n",
    "    # image data is dictionary resp['query']['pages'] where each key has the data for one image\n",
    "    for image in resp['query']['pages'].values():\n",
    "        # build up image_dict\n",
    "        image_dict = {'title' : file_names_dict[image['title']],\n",
    "                      'title_normalized': image['title'],\n",
    "                      'image_repository' : image['imagerepository'],\n",
    "                      'user' : image['imageinfo'][0]['user'],\n",
    "                      'image_url' : image['imageinfo'][0]['url'],\n",
    "                      'image_date' : image['imageinfo'][0]['extmetadata']['DateTime']['value'],\n",
    "                      'image_credit' : image['imageinfo'][0]['extmetadata']['Credit']['value'],\n",
    "                      'image_description' : image['imageinfo'][0]['extmetadata'].get('ImageDescription', np.nan),\n",
    "                      'image_license_name' : image['imageinfo'][0]['extmetadata'].get('LicenseShortName', np.nan),\n",
    "                      'image_usage_terms' : image['imageinfo'][0]['extmetadata'].get('UsageTerms', np.nan),\n",
    "                      'image_attrib_required' : image['imageinfo'][0]['extmetadata'].get('AttributionRequired', np.nan),\n",
    "                      'image_copyright' : image['imageinfo'][0]['extmetadata'].get('Copyrighted', np.nan),\n",
    "                      'image_restriction' : image['imageinfo'][0]['extmetadata'].get('Restrictions', np.nan),\n",
    "                      'image_license' : image['imageinfo'][0]['extmetadata'].get('License', np.nan)}\n",
    "        \n",
    "        # if available, add data from 'value'\n",
    "        for field in fields_with_value:\n",
    "            if isinstance(image_dict[field], dict):\n",
    "                image_dict[field] = image_dict[field]['value']\n",
    "\n",
    "        # if image has image_description it is usually html data \n",
    "        # extract text for image_description\n",
    "        if isinstance(image_dict['image_description'], str):\n",
    "            image_dict['image_description'] = BeautifulSoup(image_dict['image_description']).text\n",
    "            \n",
    "        # standardize the data in image_attrib_required and image_copyright (sample values are 'False', 'false', 'True', etc)\n",
    "        for str_data in ['image_attrib_required', 'image_copyright']:\n",
    "            image_dict[str_data] = image_dict[str_data].strip().lower()\n",
    "    \n",
    "        image_dict_list.append(image_dict)\n",
    "        \n",
    "    return image_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_data_to_db(image_dict, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add image data to wikipedia_tdih.db\n",
    "    \n",
    "    Params:\n",
    "        image_dict: dictionary of data to be added to the database\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # get license_id and user_id (update tables first, if license_id and user_id not present)\n",
    "    license_data = (image_dict['image_license_name'], image_dict['image_usage_terms'],\n",
    "                    image_dict['image_attrib_required'], image_dict['image_copyright'])\n",
    "    license_id = update_table_wiki_copyright_license(license_data, cursor, doe)\n",
    "    user_id = update_table_wiki_user(image_dict['user'], cursor, doe)\n",
    "    \n",
    "    # get image_id\n",
    "    cursor.execute('SELECT image_id FROM wiki_image WHERE image_url = ?', (image_dict['image_url'], ))\n",
    "    image_id = cursor.fetchone()[0]\n",
    "    \n",
    "    # update table wiki_image_info\n",
    "    image_data = (doe, image_id, license_id, user_id,\n",
    "                  image_dict['image_repository'], image_dict['image_date'], image_dict['image_credit'],\n",
    "                  image_dict['description'])\n",
    "    update_table_wiki_image_info(image_data, cursor, doe)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_data_log\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of link to be updated in wiki_link_data_log\n",
    "        status_code: requests.status_code from Wikipedia API\n",
    "        cursor: database_cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "    \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = (doe, link_id, status_code)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('INSERT INTO wiki_link_data_log VALUES (null,?,?,?)', data)\n",
    "        update_status = 'update_complete'\n",
    "    \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_link_data_log',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "        \n",
    "    return update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image_info(image_data, cursor, doe):\n",
    "    \"\"\"\n",
    "    Update table wiki_image_info.\n",
    "    \n",
    "    Params:\n",
    "        image_data: tuple with data for image\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    cursor.execute('INSERT INTO wiki_image_info VALUES (null,?,?,?,?,?,?,?,?)', image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_user(user_name, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update (if needed) table wiki_user\n",
    "    \n",
    "    Params:\n",
    "        user_name: wiki_user_name\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        user_id\n",
    "    \"\"\"\n",
    "    # add user to wiki_user (if not already in database)\n",
    "    cursor.execute('INSERT or IGNORE INTO wiki_user VALUES (null,?)', (user_name))\n",
    "    \n",
    "    # get user_id\n",
    "    cursor.execute('SELECT user_id FROM wiki_user WHERE user_name = ?', user_name)\n",
    "    user_id = cursor.fetchone()[0]\n",
    "    \n",
    "    return user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_copyright_license(license_data, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update (if needed) table wiki_copyright_license)\n",
    "    \n",
    "    Params:\n",
    "        license_data\n",
    "    \"\"\"\n",
    "    cursor.execute('''SELECT copyright_license_id FROM wiki_copyright_license WHERE \n",
    "                        license_name = ? AND\n",
    "                        license_description = ? AND\n",
    "                        attrib_required = ? AND\n",
    "                        copyright = ?''', license_data)\n",
    "    \n",
    "    license_id = cursor.fetchone()\n",
    "    \n",
    "    try:\n",
    "        license_id = license_id[0]\n",
    "\n",
    "    except:\n",
    "        # insert the new license type into wiki_copyright_license\n",
    "        license_data = (doe, *license_data)\n",
    "        cursor.execute('INSERT INTO wiki_copyright_license VALUES (null, ?,?,?,?,?)', license_data)\n",
    "        license_id = cursor.lastrowid\n",
    "        \n",
    "    return license_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
