{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 2 - get link data)</small>\n",
    "---\n",
    "**Goal:** get link data (i.e. page data) for links in wikipedia_tdih.db that have no data in the database or have outdated data.\n",
    "\n",
    "**Notes:**  \n",
    "- this notebook is for step two out of three of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_step_01_get_day_data_30jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)\n",
    "- the notebook for the third step is [TADS_wikipedia_tdih_main_step_03_get_image_data_10feb21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow: <small>(with checkmarks for steps done in this notebook)</small>\n",
    "- get day data\n",
    "    - get day data from wikipedia API, process it, and save it to wikipedia_tdih.db\n",
    "- get link data\n",
    "    - [x] query wikipedia_tdih.db for:\n",
    "        - [x] links that are new (i.e. in wiki_link but not in wiki_get_link_data_log)\n",
    "        - [x] links that have have not been updated since a specified_date\n",
    "    - [x] get link data from wikipedia API and extract the following:\n",
    "        - [x] page_size\n",
    "        - [x] incoming_links\n",
    "        - [x] coordinates\n",
    "        - [x] page_score\n",
    "        - [x] first_paragraphs\n",
    "        - [x] image_url\n",
    "        - [x] image_file\n",
    "        - [x] wikibase_shortdesc\n",
    "        - [x] wikibase_item\n",
    "        - [x] wiki_desc\n",
    "        - [x] page_views\n",
    "    - [x] update wikipedia_tdih.db tables\n",
    "        - [x] wiki_link_data_log\n",
    "        - [x] wiki_link_info\n",
    "        - [x] wiki_link_page_views\n",
    "        - [x] wiki_image <small>*</small>\n",
    "        - [x] wiki_image_usage <small>*</small>\n",
    "- get image data\n",
    "    - get image data from wikipedia API, process it, and save it to wikipedia_tdih.db  \n",
    "  \n",
    "<small>* wiki_image and wiki_image_usage are updated in this step (step two) only with data that shows relationship between wiki_link and wiki_image. The image data from Wikipedia is retrieved, processed, and saved to the database in step three.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import config\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = config.DATABASE_FILE\n",
    "DOE = config.DOE\n",
    "URL = config.WIKIPEDIA_URL\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'cirrusbuilddoc|cirruscompsuggestbuilddoc|extracts|pageimages|pageprops|pageterms|pageviews',\n",
    "          'piprop': 'original',\n",
    "          'exintro': True # gives the first paragraphs of a page (before detail sections)\n",
    "         }\n",
    "# explanation of values for 'prop'\n",
    "# cirrusbuilddoc for text_bytes (aka page_size), incoming_link, coordinates (if any)\n",
    "# cirruscompsuggestbuilddoc for score\n",
    "# extracts (+ exintro) for first_paragraph(s) (regex to clean up) (might me better to use ['cirrusbuilddoc']['text_source'])\n",
    "# pageimages for pageimage (with piprop='original') for full size image (but doesn't return image file anymore)\n",
    "# pageprops for page_image_free, wikibase_short_description, wikibase_item\n",
    "# pageterms for description (similar but not identital to wikibase_short_description)\n",
    "# pageviews for pageviews :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE, print_progress = 0,\n",
    "                   sleep_range = (5,10), url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the link data for a group of links in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_links_to_update_from_db\n",
    "        - get_link_data\n",
    "        - extract_link_data\n",
    "        - add_link_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file: database file to save the data to\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "        batch_size: no. of links to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        doe: date of entry (defaults to current day)\n",
    "        print_progress: if different than 0, prints batch number being processed (every print_progress no. of batches)\n",
    "        sleep_range: tuple of integers indicating the lower_limit and upper_limit of how long to wait between\n",
    "                     API calls\n",
    "        url: url to use for request to wikipedia API\n",
    "        params: params to use in request to wikipedia API\n",
    "        headers: headers to use in request to wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        links_list: list of links we needed Wikipedia data for\n",
    "        failed_requests: how many requests to Wikipedia API didn't have requests.codes.ok (break at 5)\n",
    "        missing_pages: list of pages no data was received for\n",
    "        update_results: a list of dictionaries with the status of each table update\n",
    "                        e.g. {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_table_name,\n",
    "                              'update_status': 'update_complete',\n",
    "                              'update_note': np.nan}\n",
    "    \"\"\"\n",
    "    # keep track of pages no data was received for and of failed requests\n",
    "    missing_pages = []\n",
    "    failed_request = 0\n",
    "    \n",
    "    # initialize update_results\n",
    "    update_results = []\n",
    "    \n",
    "    with sqlite3.connect(database_file) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the links we need data for\n",
    "        links_list = get_links_to_update_from_db(cursor, date)\n",
    "        \n",
    "        # if print_progress\n",
    "        if print_progress:\n",
    "            print(f'links to update: {len(links_list)}')\n",
    "        links_list = links_list[:11] # keep this here only while testing\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not links_list:\n",
    "            return 'no link data needs to be retrieved / updated'\n",
    "        \n",
    "        # keep track of failed_request (break if failed_requests > 5)\n",
    "        \n",
    "    \n",
    "        # process links in batches of size batch_size\n",
    "        for i in range(0, len(links_list), batch_size):\n",
    "            # slice links_list into batches\n",
    "            link_batch = links_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # if print_progress is given\n",
    "            if print_progress:\n",
    "                if i % print_progress:\n",
    "                    print(f'processing batch {i+1} of {len(links_list)}')\n",
    "                       \n",
    "            # get link data for links in batch\n",
    "            resp = get_link_data(link_batch, url = url, params = params, headers = headers)\n",
    "            \n",
    "            # extract link_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                page_dicts, missing_batch_pages = extract_link_data(resp, link_batch)\n",
    "                \n",
    "                # if missing_batch_pages list is not empty, update missing_pages\n",
    "                missing_pages.extend(missing_batch_pages)\n",
    "\n",
    "            else:\n",
    "                # update wiki_link_data_log with response status_code\n",
    "                update_results.append(update_table_wiki_link_data_log(link_batch, resp.status_code))\n",
    "                failed_request += 1\n",
    "                if failed_request == 5:\n",
    "                    print('failed requests limit reached.')\n",
    "                    return links_list, failed_requests, missing_pages\n",
    "                continue\n",
    "\n",
    "            # add link_data to db\n",
    "            for page_dict in page_dicts:\n",
    "                update_results.extend(add_link_data_to_db(page_dict, database_file, doe))\n",
    "                \n",
    "            # sleep time before the next API call\n",
    "            time.sleep(round(random.uniform(sleep_range[0], sleep_range[1]), 2))\n",
    "            \n",
    "    return links_list, failed_request, missing_pages, update_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_update_from_db(database_cursor, date = ''):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the links for which wikipedia data is needed.\n",
    "    \n",
    "    There are two cases in which data is needed for a specific link:\n",
    "        1. the link has just been added to the wikipedia_tdih.db and link data has not been requested from \n",
    "           wikipedia API yet\n",
    "        2. the existing data for the link needs to be updated\n",
    "    \n",
    "    Params:\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "              \n",
    "    Returns:\n",
    "        links_list: list of links for which wikpedia data is needed\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = database_cursor\n",
    "    \n",
    "    # if no date is given\n",
    "    if not date:\n",
    "        # select the links that are in wiki_link but not in wiki_get_link_data_log\n",
    "        # (these are links which have just been added to wiki_link)\n",
    "        links_list = cursor.execute('''SELECT link_id, link_title FROM wiki_link WHERE link_id NOT IN (\n",
    "                                       SELECT link_id FROM wiki_link_data_log) ''').fetchall()\n",
    "    \n",
    "    # if date is given\n",
    "    else:\n",
    "        links_list = cursor.execute('''SELECT link_id, link_title FROM wiki_link WHERE link_id IN (\n",
    "                                       SELECT link_id FROM wiki_get_link_data_log WHERE doe > ?)''', (date,)).fetchall()\n",
    "      \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get link data from Wikipedia API. Links are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        link_batch: list of tuples representing links to get data for (usually a batch of size 5)\n",
    "                    each tuple is of the form (link_id, link_title)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "        \n",
    "    Returns:\n",
    "        resp: the response object\n",
    "    \"\"\"\n",
    "    # join the titles for the links in the list link_batch and assign them to request params\n",
    "    # wikipedia uses these titles as identifiers for its pages\n",
    "    titles = '|'.join(wiki_link[1] for wiki_link in link_batch)\n",
    "\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_data(response, link_batch):\n",
    "    \"\"\"\n",
    "    Extract link data\n",
    "    \n",
    "    Params:\n",
    "        response: response data received from Wikipedia API\n",
    "        link_batch: list of titles for the links data was requested for\n",
    "        \n",
    "    Returns:\n",
    "        page_dicts: list of dictionaries with page data\n",
    "    \n",
    "    \"\"\"\n",
    "    # the needed response data is in response.json()['query']['pages']\n",
    "    resp = response.json()['query']['pages']\n",
    "    status_code = response.status_code\n",
    "    page_dicts = []\n",
    "    missing_pages = []\n",
    "\n",
    "    print(resp.keys())\n",
    "\n",
    "    for page_id in resp:\n",
    "        print(resp[page_id]['title'])\n",
    "    \n",
    "    for page_id in resp:\n",
    "        \n",
    "        # if page_id is negative number => page doesn't exist (possibly, title is wrong)\n",
    "        # for each page that doesn't exist, the page_id decreases by one\n",
    "        if int(page_id) < 0:\n",
    "            # if page_id == -1, compare titles requested vs. titles received, and return missing ones\n",
    "            # this only has to be done once, that's why we ignore page_ids less than -1\n",
    "            if int(page_id) == -1:\n",
    "                missing_pages = list(set(link_batch) - set(resp.keys()))\n",
    "            continue\n",
    "                    \n",
    "        # for pages with valid data        \n",
    "        page_dict = resp[page_id]\n",
    "        \n",
    "        # TO-DO cover for redirects\n",
    "        page_redirect = list(page_dict['cirruscompsuggestbuilddoc'].keys())[0]\n",
    "        \n",
    "        # TO-DO function for getting needed values\n",
    "        try: image_url = page_dict['original']['source']\n",
    "        except: image_url = np.nan\n",
    "            \n",
    "        try: image_file = page_dict['pageprops']['page_image_free']\n",
    "        except: image_file = np.nan\n",
    "            \n",
    "        try: wikibase_shortdesc = page_dict['pageprops']['wikibase-shortdesc']\n",
    "        except: wikibase_shortdesc = np.nan\n",
    "            \n",
    "        try: wikibase_item = page_dict['pageprops']['wikibase_item']\n",
    "        except: wikibase_item = np.nan\n",
    "            \n",
    "        try: wiki_desc = page_dict['terms']['description']\n",
    "        except: wiki_desc = np.nan\n",
    "        \n",
    "        # build up the data dict\n",
    "        data_dict = {'link_title': page_dict['title'], # change this so links saved to db are stripped of '/wiki/'?\n",
    "                     'status_code': status_code,\n",
    "                     'created_at': page_dict['cirrusbuilddoc']['create_timestamp'],\n",
    "                     'page_size': page_dict['cirrusbuilddoc']['text_bytes'],\n",
    "                     'incoming_links': page_dict['cirrusbuilddoc']['incoming_links'],\n",
    "                     'coordinates': page_dict['cirrusbuilddoc'].get('coordinates', np.nan),\n",
    "                     'page_score': page_dict['cirruscompsuggestbuilddoc'][page_redirect]['score_explanation']['value'],\n",
    "                     'first_paragraphs': BeautifulSoup(page_dict['extract']).text.strip(),\n",
    "                     'image_url': image_url,\n",
    "                     'image_file': image_file,\n",
    "                     'wikibase_shortdesc': wikibase_shortdesc,\n",
    "                     'wikibase_item': wikibase_item,\n",
    "                     'wiki_desc': wiki_desc,\n",
    "                     'page_views': page_dict['pageviews']}\n",
    "        page_dicts.append(data_dict)\n",
    "        \n",
    "    return page_dicts, missing_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_link_data_to_db(page_data_dict, database_file = DATABASE_FILE, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add wiki_link data to wikipedia_tdih.db\n",
    "    \n",
    "    Params:\n",
    "        page_data_dict: dictionary with data about wiki_link\n",
    "                        e.g. page_data_dict = {'link_title': ,\n",
    "                                               'status_code': ,\n",
    "                                               'created_at': ,\n",
    "                                               'page_size': ,\n",
    "                                               'incoming_links': ,\n",
    "                                               'coordinates': ,\n",
    "                                               'page_score': ,\n",
    "                                               'first_paragraphs': ,\n",
    "                                               'image_url': ,\n",
    "                                               'image_file': ,\n",
    "                                               'wikibase_shortdesc': ,\n",
    "                                               'wikibase_item': ,\n",
    "                                               'wiki_desc': ,\n",
    "                                               'page_views': }\n",
    "        database_file: database_file to save data to\n",
    "        doe: data of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        update_results: a list of dictionaries with the status of each table update\n",
    "                        e.g. {'doe': doe,\n",
    "                              'wiki_table_name': 'wiki_event',\n",
    "                              'update_status': 'update_complete',\n",
    "                              'update_note': 'events'}\n",
    "        \n",
    "    \"\"\"\n",
    "    update_results = []\n",
    "    no_image_for = []\n",
    "    # connect to wikipedia_tdih.db\n",
    "    with sqlite3.connect(database_file) as conn:\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "        # get link_id from db\n",
    "        cursor.execute('SELECT link_id, link_title FROM wiki_link WHERE link_title = ?', (page_data_dict['link_title'],))\n",
    "        link_id = cursor.fetchone()[0]\n",
    "    \n",
    "        # update wiki_link tables and append results to update_results\n",
    "        update_results.append(update_table_wiki_link_data_log(link_id, page_data_dict['status_code'], cursor, doe))\n",
    "        update_results.append(update_table_wiki_link_page_views(link_id, page_data_dict['page_views'], cursor, doe))\n",
    "        update_results.append(update_table_wiki_link_info(link_id, page_data_dict, cursor, doe))\n",
    "        \n",
    "        # get the image_url and image_file\n",
    "        image_url = page_data_dict['image_url']\n",
    "        image_file = page_data_dict['image_file']\n",
    "        # if there's no image for the page, update list no_image_for \n",
    "        if not np.iterable(image_url) or not np.iterable(image_file):\n",
    "            no_image_for = [link_id]\n",
    "        # else, update image tables\n",
    "        else:\n",
    "            image_id, image_update = update_table_wiki_image(image_url, image_file, cursor, doe)\n",
    "            update_results.append(image_update)\n",
    "            update_results.append(update_table_wiki_image_usage(image_id, link_id, cursor, doe))\n",
    "            \n",
    "    return update_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_data_log\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of link to be updated in wiki_link_data_log\n",
    "        status_code: requests.status_code from Wikipedia API\n",
    "        cursor: database_cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "    \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    \n",
    "    data = (doe, link_id, status_code)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute('INSERT INTO wiki_link_data_log VALUES (null,?,?,?)', data)\n",
    "        update_status = 'update_complete'\n",
    "    \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_link_data_log',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "        \n",
    "    return update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_page_views(link_id, page_views, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_views: dictionary of page view data to update\n",
    "        doe: date of entry\n",
    "        \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    # build up data list\n",
    "    data = [(doe, link_id, date, views) for date, views in page_views.items()]\n",
    "    \n",
    "    try:\n",
    "        cursor.executemany('INSERT INTO wiki_link_page_views VALUES (null,?,?,?,?)', data)\n",
    "        update_status = 'update_complete'\n",
    "    \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_link_page_views',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': link_id}\n",
    "        \n",
    "    return update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_info(link_id, page_dict, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_data_dict: dictionary with data retrieved from Wikipedia API\n",
    "                        eg. page_dict = {'link_title': link_title,\n",
    "                                         'status_code': status_code,\n",
    "                                         'created_at': created_at,\n",
    "                                         'page_size': page_size,\n",
    "                                         'incoming_links': incoming_links,\n",
    "                                         'coordinates': coordinates,\n",
    "                                         'page_score': page_score,\n",
    "                                         'first_paragraphs': first_paragraphs,\n",
    "                                         'image_url': image_url,\n",
    "                                         'image_file': image_file,\n",
    "                                         'wikibase_shortdesc': wikibase_short_desc,\n",
    "                                         'wikibase_item': wikibase_item,\n",
    "                                         'wiki_desc': wiki_desc,\n",
    "                                         'page_views': page_views}\n",
    "        cursor: database cursor\n",
    "        doe: date of entry\n",
    "        \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    # format created_at from string like '2008-03-01T03:13:01Z' to '2008-03-01'\n",
    "    created_at = page_dict['created_at'].lower().replace('z','')\n",
    "    created_at = dt.datetime.fromisoformat(created_at).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # build up data tuple\n",
    "    data = (doe, \n",
    "            link_id, \n",
    "            created_at,\n",
    "            int(page_dict['page_size']),\n",
    "            int(page_dict['incoming_links']), \n",
    "            str(page_dict['coordinates']), \n",
    "            int(page_dict['page_score']),\n",
    "            page_dict['first_paragraphs'], \n",
    "            page_dict['wikibase_shortdesc'],\n",
    "            page_dict['wikibase_item'],\n",
    "            str(page_dict['wiki_desc']))\n",
    "    \n",
    "    try:\n",
    "#         print('wiki_link_info')\n",
    "#         for d in data:\n",
    "#             print(type(d), ': ', d)\n",
    "#         print('---')\n",
    "        # insert data into wiki_link_info\n",
    "        cursor.execute('INSERT INTO wiki_link_info VALUES (null,?,?,?,?,?,?,?,?,?,?,?)', data)\n",
    "        update_status = 'update_complete'\n",
    "        \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_link_info',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "        \n",
    "    return update_status_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image(image_url, image_file, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image if image is not already in the table.\n",
    "    \n",
    "    Params:\n",
    "        image_url: wikipedia url for image used for wiki_link\n",
    "        image_file: wikipedia file name for image used for wiki_link\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        image_id\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # insert image_url if not already in wiki_image    \n",
    "        cursor.execute('INSERT OR IGNORE INTO wiki_image VALUES (null,?,?,?)', (doe, image_url, image_file))\n",
    "\n",
    "        # get the image_id (to update wiki_image_usage)\n",
    "        cursor.execute('SELECT image_id FROM wiki_image WHERE image_url = ?', (image_url,))\n",
    "        image_id = cursor.fetchone()[0]\n",
    "        \n",
    "        update_status = 'update_complete'\n",
    "        \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        image_id = np.nan\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_image',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "    \n",
    "    # return image_id\n",
    "    return image_id, update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image_usage(image_id, link_id, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image_usage.\n",
    "    \n",
    "    Params:\n",
    "        image_id: id of image used by wiki_link (primary key in wiki_image)\n",
    "        link_id: id of wiki_link (primary key in wiki_link)\n",
    "        \n",
    "    Returns:\n",
    "        update_status_dict: dictionary with status of wiki_day_data_log update\n",
    "                            e.g. {'doe': doe,\n",
    "                                  'wiki_table_name': 'wiki_day_data_log',\n",
    "                                  'update_status': update_status\n",
    "                                  'update_note': np.nan}  # this is relevant to other tables (e.g. wiki_event)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # insert data into wiki_image_usage\n",
    "        cursor.execute('INSERT INTO wiki_image_usage VALUES (null,?,?,?)', (doe, image_id, link_id))\n",
    "        update_status = 'update_complete'\n",
    "    \n",
    "    except Exception as e:\n",
    "        update_status = repr(e)\n",
    "        \n",
    "    update_status_dict = {'doe': doe,\n",
    "                          'wiki_table_name': 'wiki_image_usage',\n",
    "                          'update_status': update_status,\n",
    "                          'update_note': np.nan}\n",
    "    \n",
    "    return update_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links to update: 290\n",
      "dict_keys(['1417207', '863', '345887', '193600', '55694068'])\n",
      "Alaungpaya\n",
      "American Civil War\n",
      "Bar Confederation\n",
      "Jay Treaty\n",
      "Konbaung Dynasty\n",
      "dict_keys(['49596931', '35019234', '53274', '84504', '3434750'])\n",
      "Kilpatrick–Dahlgren Raid\n",
      "Piedra Movediza\n",
      "Richmond, Virginia\n",
      "St. Petersburg, Florida\n",
      "United States\n",
      "dict_keys(['918969'])\n",
      "Tandil\n"
     ]
    }
   ],
   "source": [
    "links_list, failed_request, missing_pages, update_results = link_data_main(database_file = DATABASE_FILE, \n",
    "                                                                            date = '', batch_size = 5, \n",
    "                                                                            doe = DOE, print_progress = 1,\n",
    "                                                                            sleep_range = (5,10), url = URL, \n",
    "                                                                            params = PARAMS, headers = HEADERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
