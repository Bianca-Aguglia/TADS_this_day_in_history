{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 2 - get link data)</small>\n",
    "---\n",
    "**Goal:** get link data (i.e. page data) for links in wikipedia_tdih.db that have no data in the database or have outdated data.\n",
    "\n",
    "**Notes:**  \n",
    "- this notebook is for step two out of three of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_step_01_get_day_data_30jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)\n",
    "- the notebook for the third step is [TADS_wikipedia_tdih_main_step_03_get_image_data_10feb21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow: <small>(with checkmarks for steps done in this notebook)</small>\n",
    "- get day data\n",
    "    - get day data from wikipedia API, process it, and save it to wikipedia_tdih.db\n",
    "- get link data\n",
    "    - [x] query wikipedia_tdih.db for:\n",
    "        - [x] links that are new (i.e. in wiki_link but not in wiki_get_link_data_log)\n",
    "        - [x] links that have have not been updated since a specified_date\n",
    "    - [x] get link data from wikipedia API and extract the following:\n",
    "        - [x] page_size\n",
    "        - [x] incoming_links\n",
    "        - [x] coordinates\n",
    "        - [x] page_score\n",
    "        - [x] first_paragraphs\n",
    "        - [x] image_url\n",
    "        - [x] image_file\n",
    "        - [x] wikibase_shortdesc\n",
    "        - [x] wikibase_item\n",
    "        - [x] wiki_desc\n",
    "        - [x] page_views\n",
    "    - [x] update wikipedia_tdih.db tables\n",
    "        - [x] wiki_link_data_log\n",
    "        - [x] wiki_link_info\n",
    "        - [x] wiki_link_page_views\n",
    "        - [x] wiki_image <small>*</small>\n",
    "        - [x] wiki_image_usage <small>*</small>\n",
    "- get image data\n",
    "    - get image data from wikipedia API, process it, and save it to wikipedia_tdih.db  \n",
    "  \n",
    "<small>* wiki_image and wiki_image_usage are updated in this step (step two) only with data that shows relationship between wiki_link and wiki_image. The image data from Wikipedia is retrieved, processed, and saved to the database in step three.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import config\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = config.DATABASE_FILE\n",
    "DOE = config.DOE\n",
    "URL = config.WIKIPEDIA_URL\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'cirrusbuilddoc|cirruscompsuggestbuilddoc|extracts|pageimages|pageprops|pageterms|pageviews',\n",
    "          'piprop': 'original',\n",
    "          'exintro': True # gives the first paragraphs of a page (before detail sections)\n",
    "         }\n",
    "# explanation of values for 'prop'\n",
    "# cirrusbuilddoc for text_bytes (aka page_size), incoming_link, coordinates (if any)\n",
    "# cirruscompsuggestbuilddoc for score\n",
    "# extracts (+ exintro) for first_paragraph(s) (regex to clean up) (might me better to use ['cirrusbuilddoc']['text_source'])\n",
    "# pageimages for pageimage (with piprop='original') for full size image (but doesn't return image file anymore)\n",
    "# pageprops for page_image_free, wikibase_short_description, wikibase_item\n",
    "# pageterms for description (similar but not identital to wikibase_short_description)\n",
    "# pageviews for pageviews :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE, print_progress = 0,\n",
    "                   sleep_range = (5,10), url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the link data for a group of links in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_links_to_update_from_db\n",
    "        - get_link_data\n",
    "        - extract_link_data\n",
    "        - add_link_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file: database file to save the data to\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "        batch_size: no. of links to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        doe: date of entry (defaults to current day)\n",
    "        print_progress: if different than 0, prints batch number being processed (every print_progress no. of batches)\n",
    "        sleep_range: tuple of integers indicating the lower_limit and upper_limit of how long to wait between\n",
    "                     API calls\n",
    "        url: url to use for request to wikipedia API\n",
    "        params: params to use in request to wikipedia API\n",
    "        headers: headers to use in request to wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(DATABASE_FILE) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the links we need data for\n",
    "        links_list = get_links_to_update_from_db(cursor, date)\n",
    "        links_list = links_list[:11] # keep this here only while testing\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not links_list:\n",
    "            return 'no link data needs to be retrieved / updated'\n",
    "        \n",
    "        # keep track of failed_request (break if failed_requests > 5)\n",
    "        failed_request = 0\n",
    "    \n",
    "        # process links in batches of size batch_size\n",
    "        for i in range(0, len(links_list), batch_size):\n",
    "            # slice links_list into batches\n",
    "            link_batch = links_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # if print_progress is given\n",
    "            if print_progress:\n",
    "                if i % print_progress:\n",
    "                    print(f'processing batch {i+1} of {len(links_list)}')\n",
    "                       \n",
    "            # get link data for links in batch\n",
    "            resp = get_link_data(link_batch, url = url, params = params, headers = headers)\n",
    "            \n",
    "            # extract link_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                page_dicts = extract_link_data(resp)\n",
    "\n",
    "            else:\n",
    "                # update wiki_link_data_log with response status_code\n",
    "#                 update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE)\n",
    "                update_table_wiki_link_data_log(link_batch, resp.status_code)\n",
    "                failed_request += 1\n",
    "                if failed_request % 5 == 0:\n",
    "                    return f'failed requests: {failed_request}'\n",
    "                continue\n",
    "\n",
    "            # add link_data to db\n",
    "            for page_dict in page_dicts:\n",
    "                add_link_data_to_db()\n",
    "                \n",
    "            # sleep time before the next API call\n",
    "            time.sleep(round(random.uniform(sleep_range[0], sleep_range[1]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_update_from_db(database_cursor, date = ''):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the links for which wikipedia data is needed.\n",
    "    \n",
    "    There are two cases in which data is needed for a specific link:\n",
    "        1. the link has just been added to the wikipedia_tdih.db and link data has not been requested from \n",
    "           wikipedia API yet\n",
    "        2. the existing data for the link needs to be updated\n",
    "    \n",
    "    Params:\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "              \n",
    "    Returns:\n",
    "        links_list: list of links for which wikpedia data is needed\n",
    "    \"\"\"\n",
    "    \n",
    "    c = database_cursor\n",
    "    \n",
    "    # if no date is given\n",
    "    if not date:\n",
    "        # select the links that are in wiki_link but not in wiki_get_link_data_log\n",
    "        # (these are links which have just been added to wiki_link)\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id NOT IN (\n",
    "                                    SELECT link_id FROM wiki_link_data_log) ''').fetchall()\n",
    "    \n",
    "    # if date is given\n",
    "    else:\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id IN (\n",
    "                                    SELECT link_id FROM wiki_get_link_data_log WHERE doe > ?)''', (date,)).fetchall()\n",
    "      \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get link data from Wikipedia API. Links are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        link_batch: list of links to get data for (usually a batch of size 5)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "    \"\"\"\n",
    "    # join the urls for the links in the list link_batch and assign them to request params\n",
    "    # wikipedia uses these urls as titles\n",
    "    # each link is a tuple in format (link_id, link_url)\n",
    "    # link url needs to be stripped of '/wiki/'\n",
    "    # e.g. link url '/wiki/Albert_Einstein' needs to be changed to 'Albert_Einstein'\n",
    "    titles = '|'.join(wiki_link[1].replace('/wiki/', '') for wiki_link in link_batch)\n",
    "    print(titles)\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_data(response):\n",
    "    \"\"\"\n",
    "    Extract link data\n",
    "    \n",
    "    Params:\n",
    "        response: response data received from Wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        page_dicts: list of dictionaries with page data\n",
    "    \n",
    "    \"\"\"\n",
    "    resp = response.json()\n",
    "    status_code = response.status_code\n",
    "    page_dicts = []\n",
    "    \n",
    "    # get the wiki_links from the response (to match them to wiki_link in database)\n",
    "    wiki_links_list = resp['query']['normalized']\n",
    "    wiki_links_dict = {link['to']: link['from'] for link in wiki_links_list}\n",
    "    \n",
    "    print(wiki_links_list)\n",
    "    print(wiki_links_dict)\n",
    "    print(resp['query']['pages'].keys())\n",
    "    for page_id in resp['query']['pages'].keys():\n",
    "        print(resp['query']['pages'][page_id]['title'])\n",
    "    \n",
    "    for page_id in resp['query']['pages'].keys():\n",
    "        page_dict = resp['query']['pages'][page_id]\n",
    "        \n",
    "        # wiki_link is normalized by Wikipedia (e.g. 'Albert_Einstein' normalized to 'Albert Einstein')\n",
    "        # extract it and find its original value\n",
    "        wiki_link_normalized = page_dict['title']\n",
    "        print(wiki_link_normalized)\n",
    "        wiki_link = wiki_links_dict.get('wiki_link_normalized', wiki_link_normalized)\n",
    "#         wiki_link = wiki_links_dict[wiki_link_normalized]\n",
    "        \n",
    "        # build up the data dict\n",
    "        data_dict = {'wiki_link': f'/wiki/{wiki_link}', # change this so links saved to db are stripped of '/wiki/'?\n",
    "                     'status_code': status_code,\n",
    "                     'page_size': page_dict['cirrusbuilddoc']['text_bytes'],\n",
    "                     'incoming_links': page_dict['cirrusbuilddoc']['incoming_links'],\n",
    "                     'coordinates': page_dict['cirrusbuilddoc']['coordinates'],\n",
    "                     'page_score': page_dict['cirruscompsuggestbuilddoc'][f'{page_id}t']['score_explanation']['value'],\n",
    "                     'first_paragraphs': BeautifulSoup(page_dict['extract']).text.strip(),\n",
    "                     'image_url': page_dict['original']['source'],\n",
    "                     'image_file': page_dict['pageprops']['page_image_free'],\n",
    "                     'wikibase_shortdesc': page_dict['pageprops']['wikibase-shortdesc'],\n",
    "                     'wikibase_item': page_dict['pageprops']['wikibase_item'],\n",
    "                     'wiki_desc': page_dict['terms']['description'],\n",
    "                     'page_views': page_dict['pageviews']}\n",
    "        page_dicts.append(data_dict)\n",
    "        \n",
    "    return page_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_link_data_to_db(page_data_dict, db = DATABASE_FILE, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add wiki_link data to wikipedia_tdih.db\n",
    "    \"\"\"\n",
    "    # connect to wikipedia_tdih.db\n",
    "    with sqlite3.connect(db) as conn:\n",
    "        c = conn.cursor()\n",
    "    \n",
    "        # get link_id from db\n",
    "        c.execute('SELECT link_id, link_url FROM wiki_link WHERE link_url = ?', (page_data_dict[wiki_link],))\n",
    "        link_id = c.fetchone()[0]\n",
    "    \n",
    "        # update wiki_link tables\n",
    "        update_table_wiki_link_data_log(link_id, page_data_dict['status_code'], c, doe)\n",
    "        update_table_wiki_link_page_views(link_id, page_data_dict['page_views'], c, doe)\n",
    "        update_table_wiki_link_info(link_id, page_data_dict, c, doe)\n",
    "        \n",
    "        # update wiki_image_tables\n",
    "        image_id = update_table_wiki_image(page_data_dict['image_url'], page_data_dict['image_file'], c, doe)\n",
    "        update_table_wiki_image_usage(image_id, link_id, c, doe)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_data_log\n",
    "    \"\"\"\n",
    "    \n",
    "    data = (doe, link_id, status_code)\n",
    "    \n",
    "    cursor.execute('INSERT INTO wiki_link_data_log VALUES (null,?,?,?)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_page_views(link_id, page_views, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_views: dictionary of page view data to update\n",
    "        doe: date of entry\n",
    "    \"\"\"\n",
    "    data = [(doe, link_id, date, views) for date, views in page_views.items()]\n",
    "    \n",
    "    cursor.executemany('INSERT INTO wiki_link_size VALUES (null,?,?,?)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_info(link_id, page_dict, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_data_dict: dictionary with data retrieved from Wikipedia API\n",
    "                        eg. page_dict = {'wiki_link': wiki_link,\n",
    "                                         'status_code': status_code,\n",
    "                                         'page_size': page_size,\n",
    "                                         'incoming_links': incoming_links,\n",
    "                                         'coordinates': coordinates,\n",
    "                                         'page_score': page_score,\n",
    "                                         'first_paragraphs': first_paragraphs,\n",
    "                                         'image_url': image_url,\n",
    "                                         'image_file': image_file,\n",
    "                                         'wikibase_short_desc': wikibase_short_desc,\n",
    "                                         'wikibase_item': wikibase_item,\n",
    "                                         'wiki_desc': wiki_desc,\n",
    "                                         'page_views': page_views}\n",
    "        cursor: database cursor\n",
    "        doe: date of entry\n",
    "        \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # build up data tuple\n",
    "    data = (doe, \n",
    "            link_id,\n",
    "            page_dict['page_size'],\n",
    "            page_dict['incoming_links'], \n",
    "            page_dict['coordinates'], \n",
    "            page_dict['page_score'],\n",
    "            page_dict['first_paragraphs'], \n",
    "            page_dict['wikibase_short_description'],\n",
    "            page_dict['wikibase_item'],\n",
    "            page_dict['wiki_desc'])\n",
    "    \n",
    "    # insert data into wiki_link_info\n",
    "    cursor.execute('INSERT INTO wiki_link_info VALUES (null,?,?,?,?,?,?,?,?,?,?)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image(image_url, image_file, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image if image is not already in the table.\n",
    "    \n",
    "    Params:\n",
    "        image_url: wikipedia url for image used for wiki_link\n",
    "        image_file: wikipedia file name for image used for wiki_link\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        image_id\n",
    "    \"\"\"\n",
    "    # insert image_url if not already in wiki_image    \n",
    "    cursor.execute('INSERT OR IGNORE INTO wiki_image VALUES (null,?,?)', (doe, image_url, image_file))\n",
    "    \n",
    "    # get the image_id (to update wiki_image_usage)\n",
    "    cursor.execute('SELECT image_id FROM wiki_image WHERE image_url = ?', (image_url,))\n",
    "    image_id = c.fetchone()[0]\n",
    "    \n",
    "    # return image_id\n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image_usage(image_id, link_id, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image_usage.\n",
    "    \n",
    "    Params:\n",
    "        image_id: id of image used by wiki_link (primary key in wiki_image)\n",
    "        link_id: id of wiki_link (primary key in wiki_link)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # insert data into wiki_image_usage\n",
    "    cursor.execute('INSERT INTO wiki_image_usage VALUES (null,?,?,?)', (doe, image_id, link_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%C3%87a%C4%9Fda%C5%9F_Atan|%C3%89va_Sz%C3%A9kely|Abel_Tasman|Academy_Awards|Adam_Sinclair\n",
      "[{'from': 'Abel_Tasman', 'to': 'Abel Tasman'}, {'from': 'Academy_Awards', 'to': 'Academy Awards'}, {'from': 'Adam_Sinclair', 'to': 'Adam Sinclair'}]\n",
      "{'Abel Tasman': 'Abel_Tasman', 'Academy Awards': 'Academy_Awards', 'Adam Sinclair': 'Adam_Sinclair'}\n",
      "dict_keys(['-1', '-2', '1988', '324', '2267098'])\n",
      "%C3%87a%C4%9Fda%C5%9F_Atan\n",
      "%C3%89va_Sz%C3%A9kely\n",
      "Abel Tasman\n",
      "Academy Awards\n",
      "Adam Sinclair\n",
      "%C3%87a%C4%9Fda%C5%9F_Atan\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cirrusbuilddoc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9af27b37df8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE, print_progress = 1,\n\u001b[0;32m----> 2\u001b[0;31m                sleep_range = (5,10), url = URL, params = PARAMS, headers = HEADERS)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-0937b6ba7713>\u001b[0m in \u001b[0;36mlink_data_main\u001b[0;34m(database_file, date, batch_size, doe, print_progress, sleep_range, url, params, headers)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# extract link_data (if resp.status_code == requests.codes.ok)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mpage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_link_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-e3872231a48f>\u001b[0m in \u001b[0;36mextract_link_data\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     37\u001b[0m         data_dict = {'wiki_link': f'/wiki/{wiki_link}', # change this so links saved to db are stripped of '/wiki/'?\n\u001b[1;32m     38\u001b[0m                      \u001b[0;34m'status_code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                      \u001b[0;34m'page_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cirrusbuilddoc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_bytes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                      \u001b[0;34m'incoming_links'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cirrusbuilddoc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'incoming_links'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                      \u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpage_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cirrusbuilddoc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cirrusbuilddoc'"
     ]
    }
   ],
   "source": [
    "link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE, print_progress = 1,\n",
    "               sleep_range = (5,10), url = URL, params = PARAMS, headers = HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
