{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia - this day in history <small>(step 2)</small>\n",
    "---\n",
    "**Goal:** get link data (i.e. page data) for links in wikipedia_tdih.db that have no data in the database or have outdated data.\n",
    "\n",
    "**Notes:**  \n",
    "- this notebook is for step two out of three of this project. \n",
    "- the notebook for the first step is [TADS_wikipedia_tdih_main_api_step_01_02_get_data_29jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks)\n",
    "- the notebook for the third step is [TADS_wikipedia_tdih_main_api_step_01_02_get_data_29jan21](https://github.com/Bianca-Aguglia/TADS_wikipedia_this_day_in_history_build_dataset/tree/master/notebooks) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process flow: <small>(with checkmarks for steps done in this notebook)</small>\n",
    "- get day data\n",
    "    - get day data from wikipedia API, process it, and save it to wikipedia_tdih.db\n",
    "- get link data\n",
    "    - [x] query wikipedia_tdih.db for:\n",
    "        - [x] links that are new (i.e. in wiki_link but not in wiki_get_link_data_log)\n",
    "        - [x] links that have have not been updated since a specified_date\n",
    "    - [x] get link data from wikipedia API and extract the following:\n",
    "        - [x] page_size\n",
    "        - [x] incoming_links\n",
    "        - [x] coordinates\n",
    "        - [x] page_score\n",
    "        - [x] first_paragraphs\n",
    "        - [x] image_url\n",
    "        - [x] image_file\n",
    "        - [x] wikibase_shortdesc\n",
    "        - [x] wikibase_item\n",
    "        - [x] wiki_desc\n",
    "        - [x] page_views\n",
    "    - [x] update wikipedia_tdih.db tables\n",
    "        - [x] wiki_link_data_log\n",
    "        - [x] wiki_link_info\n",
    "        - [x] wiki_link_page_views\n",
    "        - [x] wiki_image <small>*</small>\n",
    "        - [x] wiki_image_usage <small>*</small>\n",
    "- get image data\n",
    "    - get image data from wikipedia API, process it, and save it to wikipedia_tdih.db  \n",
    "  \n",
    "<small>* wiki_image and wiki_image_usage are updated in this step (step two) only with data that shows relationship between wiki_link and wiki_image. The image data from Wikipedia is retrieved, processed, and saved to the database in step three.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import time\n",
    "import config\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = 'wikipedia_tdih.db'\n",
    "DOE = dt.date.today().strftime('%Y-%m-%d')\n",
    "URL = 'https://en.wikipedia.org/w/api.php/'\n",
    "HEADERS = config.HEADERS\n",
    "PARAMS = {'titles': '', # placeholder for page\n",
    "          'action': 'query',\n",
    "          'format': 'json',\n",
    "          'prop': 'cirrusbuilddoc|cirruscompsuggestbuilddoc|extracts|pageimages|pageprops|pageterms|pageviews',\n",
    "          'piprop': 'original',\n",
    "          'exintro': True # gives the first paragraphs of a page (before detail sections)\n",
    "         }\n",
    "# explanation of values for 'prop'\n",
    "# cirrusbuilddoc for text_bytes (aka page_size), incoming_link, coordinates (if any)\n",
    "# cirruscompsuggestbuilddoc for score\n",
    "# extracts (+ exintro) for first_paragraph(s) (regex to clean up) (might me better to use ['cirrusbuilddoc']['text_source'])\n",
    "# pageimages for pageimage (with piprop='original') for full size image (but doesn't return image file anymore)\n",
    "# pageprops for page_image_free, wikibase_short_description, wikibase_item\n",
    "# pageterms for description (similar but not identital to wikibase_short_description)\n",
    "# pageviews for pageviews :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_data_main(database_file = DATABASE_FILE, date = '', batch_size = 5, doe = DOE):\n",
    "    \"\"\"\n",
    "    Main function for getting and processing the link data for a group of links in wikipedia_tdih.db.\n",
    "    It uses several helper functions to break down the process into simple steps:\n",
    "        - get_links_to_update_from_db\n",
    "        - get_link_data\n",
    "        - extract_link_data\n",
    "        - add_link_data_to_db\n",
    "    \n",
    "    Params:\n",
    "        database_file:\n",
    "        date: \n",
    "        batch_size: no. of links to request data for in a single call to wikipedia API (to reduce no. of API calls)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(DATABASE_FILE) as connection:\n",
    "        cursor = connection.cursor()\n",
    "    \n",
    "        # select from wikipedia_tdih.db the links we need data for\n",
    "        links_list = get_links_to_update_from_db(cursor, date)\n",
    "    \n",
    "        # exit if no links need to have data retrieved or updated\n",
    "        if not links_list:\n",
    "            return 'no link data needs to be retrieved / updated'\n",
    "        \n",
    "        # keep track of failed_request (break if failed_requests > 5)\n",
    "        failed_request = 0\n",
    "    \n",
    "        # process links in batches of size batch_size\n",
    "        for i in range(0, len(links_list), batch_size):\n",
    "            link_batch = links_list[slice(i, i + batch_size)]\n",
    "            \n",
    "            # get link data for links in batch\n",
    "            resp = get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS)\n",
    "            \n",
    "            # extract link_data (if resp.status_code == requests.codes.ok)\n",
    "            if resp.status_code == requests.codes.ok:\n",
    "                page_dicts = extract_link_data(resp)\n",
    "\n",
    "            else:\n",
    "                # update wiki_link_data_log with response status_code\n",
    "#                 update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE)\n",
    "                update_table_wiki_link_data_log(link_batch, resp.status_code)\n",
    "                failed_request += 1\n",
    "                if failed_request % 5 == 0:\n",
    "                    return f'failed requests: {failed_request}'\n",
    "                continue\n",
    "\n",
    "            # add link_data to db\n",
    "            for page_dict in page_dicts:\n",
    "                add_link_data_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_to_update_from_db(database_cursor, date = ''):\n",
    "    \"\"\"\n",
    "    Select from wikipedia_tdih.db the links for which wikipedia data is needed.\n",
    "    \n",
    "    There are two cases in which data is needed for a specific link:\n",
    "        1. the link has just been added to the wikipedia_tdih.db and link data has not been requested from \n",
    "           wikipedia API yet\n",
    "        2. the existing data for the link needs to be updated\n",
    "    \n",
    "    Params:\n",
    "        date: default of '' indicates that only links without data should be selected\n",
    "              if date is given, select the links that have data but data has not been updated since specified date\n",
    "              if date is given, format should be '%Y-%m-%d' (e.g. '2021-01-02' for January 2nd, 2021)\n",
    "              \n",
    "    Returns:\n",
    "        links_list: list of links for which wikpedia data is needed\n",
    "    \"\"\"\n",
    "    \n",
    "    c = database_cursor\n",
    "    \n",
    "    # if no date is given\n",
    "    if not date:\n",
    "        # select the links that are in wiki_link but not in wiki_get_link_data_log\n",
    "        # (these are links which have just been added to wiki_link)\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id NOT IN (\n",
    "                                    SELECT link_id FROM wiki_get_link_data_log) ''').fetchall()\n",
    "    \n",
    "    # if date is given\n",
    "    else:\n",
    "        links_list = c.execute('''SELECT link_id, link_url FROM wiki_link WHERE link_id IN (\n",
    "                                    SELECT link_id FROM wiki_get_link_data_log WHERE doe > ?)''', (date,)).fetchall()\n",
    "      \n",
    "    return links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_data(link_batch, url = URL, params = PARAMS, headers = HEADERS):\n",
    "    \"\"\"\n",
    "    Get link data from Wikipedia API. Links are processed in batches (usually of size 5) to reduce the number\n",
    "    of API calls.\n",
    "    \n",
    "    Params:\n",
    "        link_batch: list of links to get data for (usually a batch of size 5)\n",
    "        url: url for wikipedia API\n",
    "        params: params to use in API call\n",
    "    \"\"\"\n",
    "    # join the links in link_batch and assign them to request params\n",
    "#     titles = '|'.join(link_batch)\n",
    "    titles = '|'.join(wiki_link[1] for wiki_link in link_batch)\n",
    "    params['titles'] = titles\n",
    "    \n",
    "    # request data\n",
    "    resp = requests.get(url = url, headers = headers, params = params)\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_data(response):\n",
    "    \"\"\"\n",
    "    Extract link data\n",
    "    \n",
    "    Params:\n",
    "        response: response data received from Wikipedia API\n",
    "        \n",
    "    Returns:\n",
    "        page_dicts: list of dictionaries with page data\n",
    "    \n",
    "    \"\"\"\n",
    "    resp = response.json()\n",
    "    status_code = response.status_code\n",
    "    page_dicts = []\n",
    "    \n",
    "    # get the wiki_links from the response (to match them to wiki_link in database)\n",
    "    wiki_links_list = resp['query']['normalized']\n",
    "    wiki_links_dict = {link['to']: link['from'] for link in wiki_links_list}\n",
    "    \n",
    "    for page_id in resp['query']['pages'].keys():\n",
    "        page_dict = resp['query']['pages'][page_id]\n",
    "        \n",
    "        # wiki_link is normalized by Wikipedia (e.g. 'Albert_Einstein' normalized to 'Albert Einstein')\n",
    "        # extract it and find its original value\n",
    "        wiki_link_normalized = page_dict['title']\n",
    "        wiki_link = wiki_links_dict[wiki_link_normalized]\n",
    "        \n",
    "        # build up the data dict\n",
    "        data_dict = {'wiki_link': wiki_link,\n",
    "                     'status_code': status_code,\n",
    "                     'page_size': page_dict['cirrusbuilddoc']['text_bytes'],\n",
    "                     'incoming_links': page_dict['cirrusbuilddoc']['incoming_links'],\n",
    "                     'coordinates': page_dict['cirrusbuilddoc']['coordinates'],\n",
    "                     'page_score': page_dict['cirruscompsuggestbuilddoc'][f'{page_id}t']['score_explanation']['value'],\n",
    "                     'first_paragraphs': BeautifulSoup(page_dict['extract']).text.strip(),\n",
    "                     'image_url': page_dict['original']['source'],\n",
    "                     'image_file': page_dict['pageprops']['page_image_free'],\n",
    "                     'wikibase_shortdesc': page_dict['pageprops']['wikibase-shortdesc'],\n",
    "                     'wikibase_item': page_dict['pageprops']['wikibase_item'],\n",
    "                     'wiki_desc': page_dict['terms']['description'],\n",
    "                     'page_views': page_dict['pageviews']}\n",
    "        page_dicts.append(data_dict)\n",
    "        \n",
    "    return page_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_link_data_to_db(page_data_dict, db = DATABASE_FILE, doe = DOE):\n",
    "    \"\"\"\n",
    "    Add wiki_link data to wikipedia_tdih.db\n",
    "    \"\"\"\n",
    "    # connect to wikipedia_tdih.db\n",
    "    with sqlite3.connect(db) as conn:\n",
    "        c = conn.cursor()\n",
    "    \n",
    "        # get link_id from db\n",
    "        c.execute('SELECT link_id, link_url FROM wiki_link WHERE link_url = ?', page_data_dict[wiki_link])\n",
    "        link_id = c.fetchone()[0]\n",
    "    \n",
    "        # update wiki_link tables\n",
    "        update_table_wiki_link_data_log(link_id, page_data_dict['status_code'], c, doe)\n",
    "        update_table_wiki_link_page_views(link_id, page_data_dict['page_views'], c, doe)\n",
    "        update_table_wiki_link_info(link_id, page_data_dict, c, doe)\n",
    "        \n",
    "        # update wiki_image_tables\n",
    "        update_table_wiki_image()\n",
    "        update_table_wiki_image_usage()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_data_log(link_id, status_code, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_data_log\n",
    "    \"\"\"\n",
    "    \n",
    "    data = (doe, link_id, status_code)\n",
    "    \n",
    "    c.execute('INSERT INTO wiki_link_data_log VALUES (null,?,?,? )', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_page_views(link_id, page_views, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_views: dictionary of page view data to update\n",
    "        doe: date of entry\n",
    "    \"\"\"\n",
    "    data = [(doe, link_id, date, views) for date, views in page_views.items()]\n",
    "    \n",
    "    c.executemany('INSERT INTO wiki_link_size VALUES (null,?,?,?)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_link_info(link_id, page_dict, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_link_size\n",
    "    \n",
    "    Params:\n",
    "        link_id: id of wiki_link to update page_views for\n",
    "        page_data_dict: dictionary with data retrieved from Wikipedia API\n",
    "                        eg. page_dict = {'wiki_link': wiki_link,\n",
    "                                         'status_code': status_code,\n",
    "                                         'page_size': page_size,\n",
    "                                         'incoming_links': incoming_links,\n",
    "                                         'coordinates': coordinates,\n",
    "                                         'page_score': page_score,\n",
    "                                         'first_paragraphs': first_paragraphs,\n",
    "                                         'image_url': image_url,\n",
    "                                         'image_file': image_file,\n",
    "                                         'wikibase_short_desc': wikibase_short_desc,\n",
    "                                         'wikibase_item': wikibase_item,\n",
    "                                         'wiki_desc': wiki_desc,\n",
    "                                         'page_views': page_views}\n",
    "        cursor: database cursor\n",
    "        doe: date of entry\n",
    "        \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # build up data tuple\n",
    "    data = (doe, \n",
    "            link_id,\n",
    "            page_dict['page_size'],\n",
    "            page_dict['incoming_links'], \n",
    "            page_dict['coordinates'], \n",
    "            page_dict['page_score'],\n",
    "            page_dict['first_paragraphs'], \n",
    "            page_dict['wikibase_short_description'],\n",
    "            page_dict['wikibase_item'],\n",
    "            page_dict['wiki_desc'])\n",
    "    \n",
    "    # insert data into wiki_link_info\n",
    "    c.execute('INSERT INTO wiki_link_info VALUES (null,?,?,?,?,?,?,?,?,?,?)', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image(image_url, image_file, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image if image is not already in the table.\n",
    "    \n",
    "    Params:\n",
    "        image_url: wikipedia url for image used for wiki_link\n",
    "        image_file: wikipedia file name for image used for wiki_link\n",
    "        cursor: database cursor\n",
    "        doe: date of entry (defaults to current day)\n",
    "        \n",
    "    Returns:\n",
    "        image_id\n",
    "    \"\"\"\n",
    "    # insert image_url if not already in wiki_image    \n",
    "    c.execute('INSERT OR IGNORE INTO wiki_image VALUES (null,?,?)', (doe, image_url, image_file))\n",
    "    \n",
    "    # get the image_id (to update wiki_image_usage)\n",
    "    c.execute('SELECT image_id FROM wiki_image WHERE image_url = ?', (image_url,))\n",
    "    image_id = c.fetchone()[0]\n",
    "    \n",
    "    # return image_id\n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_table_wiki_image_usage(image_id, link_id, cursor, doe = DOE):\n",
    "    \"\"\"\n",
    "    Update table wiki_image_usage.\n",
    "    \n",
    "    Params:\n",
    "        image_id: id of image used by wiki_link (primary key in wiki_image)\n",
    "        link_id: id of wiki_link (primary key in wiki_link)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # insert data into wiki_image_usage\n",
    "    c.execute('INSERT INTO wiki_image_usage VALUES (null,?,?,?)', (doe, image_id, link_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
